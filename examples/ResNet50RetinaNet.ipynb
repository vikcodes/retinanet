{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show images inline\n",
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules when they have changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import keras\n",
    "import keras\n",
    "\n",
    "# import keras_retinanet\n",
    "from keras_retinanet.models.resnet import custom_objects\n",
    "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
    "\n",
    "# import miscellaneous modules\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# set tf backend to allow memory to grow, instead of claiming everything\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_session():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    return tf.Session(config=config)\n",
    "\n",
    "# use this environment flag to change which GPU to use\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# set the modified tf session as backend in keras\n",
    "keras.backend.tensorflow_backend.set_session(get_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load RetinaNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# adjust this to point to your downloaded/trained model\n",
    "model_path = os.path.join('..', 'snapshots', 'resnet50_coco_best_v1.2.2.h5')\n",
    "\n",
    "# load retinanet model\n",
    "model = keras.models.load_model(model_path, custom_objects=custom_objects)\n",
    "#print(model.summary())\n",
    "\n",
    "# load label to names mapping for visualization purposes\n",
    "labels_to_names = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run detection on example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing time:  6.674762010574341\n",
      "[245  96 435 360] 0 person 0.95470107\n",
      "[  0 254 312 473] 0 person 0.9043867\n",
      "[452 283 644 476] 0 person 0.77840126\n",
      "[314 196 341 301] 27 tie 0.58414274\n"
     ]
    }
   ],
   "source": [
    "# load image\n",
    "image = read_image_bgr('000000008021.jpg')\n",
    "\n",
    "# copy to draw on\n",
    "draw = image.copy()\n",
    "draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# preprocess image for network\n",
    "image = preprocess_image(image)\n",
    "image, scale = resize_image(image)\n",
    "\n",
    "# process image\n",
    "start = time.time()\n",
    "_, _, detections = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
    "print(\"processing time: \", time.time() - start)\n",
    "\n",
    "# compute predicted labels and scores\n",
    "predicted_labels = np.argmax(detections[0, :, 4:], axis=1)\n",
    "scores = detections[0, np.arange(detections.shape[1]), 4 + predicted_labels]\n",
    "\n",
    "# correct for image scale\n",
    "detections[0, :, :4] /= scale\n",
    "\n",
    "# visualize detections\n",
    "for idx, (label, score) in enumerate(zip(predicted_labels, scores)):\n",
    "    if score < 0.5:\n",
    "        continue\n",
    "    b = detections[0, idx, :4].astype(int)\n",
    "    print(b, label, labels_to_names[label], score)\n",
    "    cv2.rectangle(draw, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 3)\n",
    "    caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n",
    "    cv2.putText(draw, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 0, 0), 3)\n",
    "    cv2.putText(draw, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1.5, (255, 255, 255), 2)\n",
    "    \n",
    "# plt.figure(figsize=(15, 15))\n",
    "# plt.axis('off')\n",
    "# plt.imshow(draw)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Generation of Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 10/15081\n",
      "Count: 20/15081\n",
      "Count: 30/15081\n",
      "Count: 40/15081\n",
      "Count: 50/15081\n",
      "Count: 60/15081\n",
      "Count: 70/15081\n",
      "Count: 80/15081\n",
      "Count: 90/15081\n",
      "Count: 100/15081\n",
      "Count: 110/15081\n",
      "Count: 120/15081\n",
      "Count: 130/15081\n",
      "Count: 140/15081\n",
      "Count: 150/15081\n",
      "Count: 160/15081\n",
      "Count: 170/15081\n",
      "Count: 180/15081\n",
      "Count: 190/15081\n",
      "Count: 200/15081\n",
      "Count: 210/15081\n",
      "Count: 220/15081\n",
      "Count: 230/15081\n",
      "Count: 240/15081\n",
      "Count: 250/15081\n",
      "Count: 260/15081\n",
      "Count: 270/15081\n",
      "Count: 280/15081\n",
      "Count: 290/15081\n",
      "Count: 300/15081\n",
      "Count: 310/15081\n",
      "Count: 320/15081\n",
      "Count: 330/15081\n",
      "Count: 340/15081\n",
      "Count: 350/15081\n",
      "Count: 360/15081\n",
      "Count: 370/15081\n",
      "Count: 380/15081\n",
      "Count: 390/15081\n",
      "Count: 400/15081\n",
      "Count: 410/15081\n",
      "Count: 420/15081\n",
      "Count: 430/15081\n",
      "Count: 440/15081\n",
      "Count: 450/15081\n",
      "Count: 460/15081\n",
      "Count: 470/15081\n",
      "Count: 480/15081\n",
      "Count: 490/15081\n",
      "Count: 500/15081\n",
      "Count: 510/15081\n",
      "Count: 520/15081\n",
      "Count: 530/15081\n",
      "Count: 540/15081\n",
      "Count: 550/15081\n",
      "Count: 560/15081\n",
      "Count: 570/15081\n",
      "Count: 580/15081\n",
      "Count: 590/15081\n",
      "Count: 600/15081\n",
      "Count: 610/15081\n",
      "Count: 620/15081\n",
      "Count: 630/15081\n",
      "Count: 640/15081\n",
      "Count: 650/15081\n",
      "Count: 660/15081\n",
      "Count: 670/15081\n",
      "Count: 680/15081\n",
      "Count: 690/15081\n",
      "Count: 700/15081\n",
      "Count: 710/15081\n",
      "Count: 720/15081\n",
      "Count: 730/15081\n",
      "Count: 740/15081\n",
      "Count: 750/15081\n",
      "Count: 760/15081\n",
      "Count: 770/15081\n",
      "Count: 780/15081\n",
      "Count: 790/15081\n",
      "Count: 800/15081\n",
      "Count: 810/15081\n",
      "Count: 820/15081\n",
      "Count: 830/15081\n",
      "Count: 840/15081\n",
      "Count: 850/15081\n",
      "Count: 860/15081\n",
      "Count: 870/15081\n",
      "Count: 880/15081\n",
      "Count: 890/15081\n",
      "Count: 900/15081\n",
      "Count: 910/15081\n",
      "Count: 920/15081\n",
      "Count: 930/15081\n",
      "Count: 940/15081\n",
      "Count: 950/15081\n",
      "Count: 960/15081\n",
      "Count: 970/15081\n",
      "Count: 980/15081\n",
      "Count: 990/15081\n",
      "Count: 1000/15081\n",
      "Count: 1010/15081\n",
      "Count: 1020/15081\n",
      "Count: 1030/15081\n",
      "Count: 1040/15081\n",
      "Count: 1050/15081\n",
      "Count: 1060/15081\n",
      "Count: 1070/15081\n",
      "Count: 1080/15081\n",
      "Count: 1090/15081\n",
      "Count: 1100/15081\n",
      "Count: 1110/15081\n",
      "Count: 1120/15081\n",
      "Count: 1130/15081\n",
      "Count: 1140/15081\n",
      "Count: 1150/15081\n",
      "Count: 1160/15081\n",
      "Count: 1170/15081\n",
      "Count: 1180/15081\n",
      "Count: 1190/15081\n",
      "Count: 1200/15081\n",
      "Count: 1210/15081\n",
      "Count: 1220/15081\n",
      "Count: 1230/15081\n",
      "Count: 1240/15081\n",
      "Count: 1250/15081\n",
      "Count: 1260/15081\n",
      "Count: 1270/15081\n",
      "Count: 1280/15081\n",
      "Count: 1290/15081\n",
      "Count: 1300/15081\n",
      "Count: 1310/15081\n",
      "Count: 1320/15081\n",
      "Count: 1330/15081\n",
      "Count: 1340/15081\n",
      "Count: 1350/15081\n",
      "Count: 1360/15081\n",
      "Count: 1370/15081\n",
      "Count: 1380/15081\n",
      "Count: 1390/15081\n",
      "Count: 1400/15081\n",
      "Count: 1410/15081\n",
      "Count: 1420/15081\n",
      "Count: 1430/15081\n",
      "Count: 1440/15081\n",
      "Count: 1450/15081\n",
      "Count: 1460/15081\n",
      "Count: 1470/15081\n",
      "Count: 1480/15081\n",
      "Count: 1490/15081\n",
      "Count: 1500/15081\n",
      "Count: 1510/15081\n",
      "Count: 1520/15081\n",
      "Count: 1530/15081\n",
      "Count: 1540/15081\n",
      "Count: 1550/15081\n",
      "Count: 1560/15081\n",
      "Count: 1570/15081\n",
      "Count: 1580/15081\n",
      "Count: 1590/15081\n",
      "Count: 1600/15081\n",
      "Count: 1610/15081\n",
      "Count: 1620/15081\n",
      "Count: 1630/15081\n",
      "Count: 1640/15081\n",
      "Count: 1650/15081\n",
      "Count: 1660/15081\n",
      "Count: 1670/15081\n",
      "Count: 1680/15081\n",
      "Count: 1690/15081\n",
      "Count: 1700/15081\n",
      "Count: 1710/15081\n",
      "Count: 1720/15081\n",
      "Count: 1730/15081\n",
      "Count: 1740/15081\n",
      "Count: 1750/15081\n",
      "Count: 1760/15081\n",
      "Count: 1770/15081\n",
      "Count: 1780/15081\n",
      "Count: 1790/15081\n",
      "Count: 1800/15081\n",
      "Count: 1810/15081\n",
      "Count: 1820/15081\n",
      "Count: 1830/15081\n",
      "Count: 1840/15081\n",
      "Count: 1850/15081\n",
      "Count: 1860/15081\n",
      "Count: 1870/15081\n",
      "Count: 1880/15081\n",
      "Count: 1890/15081\n",
      "Count: 1900/15081\n",
      "Count: 1910/15081\n",
      "Count: 1920/15081\n",
      "Count: 1930/15081\n",
      "Count: 1940/15081\n",
      "Count: 1950/15081\n",
      "Count: 1960/15081\n",
      "Count: 1970/15081\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import os.path as path\n",
    "import glob\n",
    "\n",
    "def generate_annotations(super_class='*'):\n",
    "    start = time.time()\n",
    "    image_paths = []\n",
    "\n",
    "    count = 0\n",
    "#     super_class\n",
    "    for f in glob.iglob('data/{}/**/*.*'.format(super_class), recursive=True):\n",
    "#         if count > 5: break\n",
    "#         if (f.split(\".\")[-1] == \"xml\"): continue\n",
    "        image_paths.append(f)\n",
    "#         count+=1\n",
    "\n",
    "    # Setup XML\n",
    "    root = ET.Element(\"root\")\n",
    "    count = 0\n",
    "    # Iterate over images in directory\n",
    "    for  image_path in image_paths:\n",
    "        count+=1\n",
    "        if (count % 10 == 0): print(\"Count: {0}/{1}\".format(count, len(image_paths)))\n",
    "        # Load Image\n",
    "        image = read_image_bgr(image_path)\n",
    "        draw = image.copy()\n",
    "        draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Preprocess and scale\n",
    "        image = preprocess_image(image)\n",
    "        image, scale = resize_image(image)\n",
    "\n",
    "        # Detect images\n",
    "        _, _, detections = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
    "\n",
    "        # compute predicted labels and scores\n",
    "        predicted_labels = np.argmax(detections[0, :, 4:], axis=1)\n",
    "        scores = detections[0, np.arange(detections.shape[1]), 4 + predicted_labels]\n",
    "\n",
    "        # correct for image scale\n",
    "        detections[0, :, :4] /= scale\n",
    "\n",
    "        image = ET.SubElement(root, \"image\")\n",
    "        annotation_path = image_path.split(\".\")[0] + \".xml\"\n",
    "        image.set('filename', image_path)\n",
    "\n",
    "        # visualize detections\n",
    "        for idx, (label, score) in enumerate(zip(predicted_labels, scores)):\n",
    "            if score < 0.5 or label != 0: # Skip if not too confident or label != 0 [Person]\n",
    "                continue \n",
    "            b = detections[0, idx, :4].astype(int) \n",
    "            person = ET.SubElement(image, \"person\")\n",
    "            ET.SubElement(person, \"xmin\").text = str(b[0])\n",
    "            ET.SubElement(person, \"ymin\").text = str(b[1])\n",
    "            ET.SubElement(person, \"xmax\").text = str(b[2])\n",
    "            ET.SubElement(person, \"ymax\").text = str(b[3])\n",
    "\n",
    "            # Displaying Bounding Box\n",
    "            # cv2.rectangle(draw, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 3)\n",
    "            # caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n",
    "            # cv2.putText(draw, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 0, 0), 3)\n",
    "            # cv2.putText(draw, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1.5, (255, 255, 255), 2)\n",
    "\n",
    "            # plt.figure(figsize=(15, 15))\n",
    "            # plt.axis('off')\n",
    "            # plt.imshow(draw)\n",
    "            # plt.show()\n",
    "\n",
    "    # Output XML file\n",
    "    # print(ET.tostring(root, encoding='utf8', method='xml'))\n",
    "    tree = ET.ElementTree(root)\n",
    "    annotations_filename = 'annotations_' + super_class + '.xml'\n",
    "    open(annotations_filename, 'a').close()\n",
    "    tree.write(annotations_filename) \n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Generation time for {} images: \".format(len(image_paths)), end - start)\n",
    "    \n",
    "generate_annotations('women')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_dict(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    result = {}\n",
    "    # Iterate over the images\n",
    "    for image in root.findall('image'):\n",
    "        filename = image.get('filename')\n",
    "        # Get all persons\n",
    "        bounding_boxes = []\n",
    "        for person in image.findall('person'):\n",
    "            xmin = int(person.find('xmin').text)\n",
    "            ymin = int(person.find('ymin').text)\n",
    "            xmax = int(person.find('xmax').text)\n",
    "            ymax = int(person.find('ymax').text)\n",
    "            bounding_boxes.append((xmin, ymin, xmax, ymax))\n",
    "        result[filename] = bounding_boxes\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(xml_to_dict('annotations_women.xml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/women/women-jumpsuits/DU7RELCTafrikrea/2DU7RELCTafrikrea.jpeg', 'data/women/women-jumpsuits/DU7RELCTafrikrea/3DU7RELCTafrikrea.jpeg', 'data/women/women-jumpsuits/DU7RELCTafrikrea/0DU7RELCTafrikrea.jpeg', 'data/women/women-jumpsuits/DU7RELCTafrikrea/1DU7RELCTafrikrea.jpeg', 'data/women/women-jumpsuits/DU7RELCTafrikrea/4DU7RELCTafrikrea.jpeg']\n",
      "(216, -11, 514, 1051)\n",
      "514 1051 216 -11\n",
      "(233, 97, 459, 1000)\n",
      "459 1000 233 97\n",
      "(115, 81, 551, 1013)\n",
      "551 1013 115 81\n",
      "(221, 5, 495, 972)\n",
      "495 972 221 5\n",
      "(120, 74, 522, 999)\n",
      "522 999 120 74\n"
     ]
    }
   ],
   "source": [
    "def xml_to_crops(num_examples=5):\n",
    "    # Show only from the women's class\n",
    "    data = xml_to_dict('annotations.xml')\n",
    "    filenames = [filename for filename in data.keys()][:num_examples]\n",
    "    print(filenames)\n",
    "    \n",
    "    for filename in filenames:\n",
    "        img = cv2.imread(filename)\n",
    "        height, width, channels = img.shape\n",
    "        boxes = data[filename]\n",
    "        for box in boxes:\n",
    "            cropped_img = img[max(0, box[1]) : min(height, box[3]), max(0, box[0]): min(width, box[2])] # y, x\n",
    "            cv2.imshow(\"cropped\", cropped_img)\n",
    "            cv2.waitKey(10)\n",
    "            time.sleep(10)\n",
    "    \n",
    "#     img = cv2.imread(\"lenna.png\")\n",
    "#     crop_img = img[y:y+h, x:x+w]\n",
    "#     cv2.imshow(\"cropped\", crop_img)\n",
    "#     cv2.waitKey(0)\n",
    "#     images = []\n",
    "#     for filename in filenames:\n",
    "        \n",
    "        \n",
    "#     images.append(cropped_image)\n",
    "#     return images\n",
    "\n",
    "xml_to_crops()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
